{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch walkthrough generated by modifying & combining several tutorials:\n",
    "  - https://morvanzhou.github.io/tutorials/\n",
    "  - [CNN Visualization notebook](https://github.com/sar-gupta/convisualize_nb/blob/master/cnn-visualize.ipynb)\n",
    "\n",
    "This notebeook will walk through:\n",
    "  * Building and training a convolutional neural network (CNN) on the MNIST dataset \n",
    "  * Visualization of the resulting learned filters\n",
    "\n",
    "Dependencies (tested on):\n",
    "* torch: 0.3.0\n",
    "* torchvision\n",
    "* matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Simple CNN for MNIST digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# If you have a GPU set this to true!\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)    # reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 2               # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.001              # learning rate\n",
    "DOWNLOAD_MNIST = True   # set to False if you have downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist digits dataset\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,                        # download it if you don't have it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one example\n",
    "print(train_data.train_data.size())                 # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())               # (60000)\n",
    "i=5\n",
    "plt.imshow(train_data.train_data[i].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test data into Variable, pick 2000 samples to speed up testing\n",
    "test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1)).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\n",
    "test_y = test_data.test_labels[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "from matplotlib import cm\n",
    "try: from sklearn.manifold import TSNE; HAS_SK = True\n",
    "except: HAS_SK = False; print('Please install sklearn for layer visualization')\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "\n",
    "plt.ion()\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        b_x = Variable(x)   # batch x\n",
    "        b_y = Variable(y)   # batch y\n",
    "\n",
    "        output = cnn(b_x)[0]               # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            test_output, last_layer = cnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == test_y).sum().item() / float(test_y.size(0))\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)\n",
    "            if HAS_SK:\n",
    "                # Visualization of trained flatten layer (T-SNE)\n",
    "                tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "                plot_only = 500\n",
    "                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])\n",
    "                labels = test_y.numpy()[:plot_only]\n",
    "                plot_with_labels(low_dim_embs, labels)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 10 predictions from test data\n",
    "test_output, _ = cnn(test_x[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10].numpy(), 'real number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [todo]: Statistics/confusion matrix using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: CNN filter visualization using pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import models\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(image):\n",
    "    \"\"\"\n",
    "    input is (d,w,h)\n",
    "    converts 3D image tensor to grayscale images corresponding to each channel\n",
    "    \"\"\"\n",
    "    image = torch.sum(image, dim=0)\n",
    "    image = torch.div(image, image.shape[0])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        image = Variable(preprocess(image).unsqueeze(0).cuda())\n",
    "    else:\n",
    "        image = Variable(preprocess(image).unsqueeze(0))\n",
    "    return image\n",
    "\n",
    "\n",
    "def predict(image):\n",
    "    _, index = vgg(image).data[0].max(0)\n",
    "    return str(index[0]), labels[str(index[0].item())][1]\n",
    "    \n",
    "def deprocess(image):\n",
    "    if USE_CUDA:\n",
    "        return image * torch.Tensor([0.229, 0.224, 0.225]).cuda()  + torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "    else:\n",
    "        return image * torch.Tensor([0.229, 0.224, 0.225]) + torch.Tensor([0.485, 0.456, 0.406])\n",
    "\n",
    "def load_image(path):\n",
    "    image = Image.open(path)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Image loaded successfully\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitten_1 = load_image(\"./images/Tongue-Kitten.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=True)\n",
    "vgg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA:\n",
    "    vgg = vgg.cuda() # if you want GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = json.load(open('labels/imagenet_class_index.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kitten_2 = normalize(kitten_1)\n",
    "print(predict(kitten_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulelist = list(vgg.features.modules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Output Maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_outputs(image):\n",
    "    outputs = []\n",
    "    names = []\n",
    "    for layer in modulelist[1:]:\n",
    "        image = layer(image)\n",
    "        outputs.append(image)\n",
    "        names.append(str(layer))\n",
    "        \n",
    "    output_im = []\n",
    "    for i in outputs:\n",
    "        i = i.squeeze(0)\n",
    "        temp = to_grayscale(i)\n",
    "        output_im.append(temp.data.cpu().numpy())\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (30, 50)\n",
    "\n",
    "\n",
    "    for i in range(len(output_im)):\n",
    "        a = fig.add_subplot(8,4,i+1)\n",
    "        imgplot = plt.imshow(output_im[i])\n",
    "        plt.axis('off')\n",
    "        a.set_title(names[i].partition('(')[0], fontsize=30)\n",
    "\n",
    "    plt.savefig('layer_outputs.jpg', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs(kitten_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of each filter separately at given layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outputs(image, layer_to_visualize):\n",
    "    if layer_to_visualize < 0:\n",
    "        layer_to_visualize += 31\n",
    "    output = None\n",
    "    name = None\n",
    "    for count, layer in enumerate(modulelist[1:]):\n",
    "        image = layer(image)\n",
    "        if count == layer_to_visualize: \n",
    "            output = image\n",
    "            name = str(layer)\n",
    "    \n",
    "    filters = []\n",
    "    output = output.data.squeeze()\n",
    "    for i in range(output.shape[0]):\n",
    "        filters.append(output[i,:,:])\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "    for i in range(int(np.sqrt(len(filters))) * int(np.sqrt(len(filters)))):\n",
    "        fig.add_subplot(np.sqrt(len(filters)), np.sqrt(len(filters)),i+1)\n",
    "        imgplot = plt.imshow(filters[i])\n",
    "        plt.axis('off')\n",
    "    \n",
    "#     print(len(filters))\n",
    "#     print(filters[0].shape)\n",
    "        \n",
    "            \n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_outputs(kitten_2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_outputs(kitten_2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize weights [todo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Specific Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalise\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_saliency_map(input, label):\n",
    "    if USE_CUDA:\n",
    "        input = Variable(preprocess(input).unsqueeze(0).cuda(), requires_grad=True)\n",
    "    else:\n",
    "        input = Variable(preprocess(input).unsqueeze(0), requires_grad=True)\n",
    "    output = vgg.forward(input)\n",
    "    output[0][label].backward()\n",
    "    grads = input.grad.data.clamp(min=0)\n",
    "    grads.squeeze_()\n",
    "    grads.transpose_(0,1)\n",
    "    grads.transpose_(1,2)\n",
    "    grads = np.amax(grads.cpu().numpy(), axis=2)\n",
    "    \n",
    "    true_image = input.data\n",
    "    true_image = true_image.squeeze()\n",
    "    true_image = true_image.transpose(0,1)\n",
    "    true_image = true_image.transpose(1,2)\n",
    "    true_image = deprocess(true_image)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "\n",
    "\n",
    "    a = fig.add_subplot(1,2,1)\n",
    "    imgplot = plt.imshow(true_image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off') \n",
    "\n",
    "    a = fig.add_subplot(1,2,2)\n",
    "    imgplot = plt.imshow(grads)\n",
    "    plt.axis('off') \n",
    "    plt.title('Saliency Map')\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = load_image('images/Golden_retr.jpg')\n",
    "dog_sal = make_saliency_map(dog, 207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldfish = load_image('images/goldfish.jpg')\n",
    "goldfish_sal = make_saliency_map(goldfish, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_grad(input, label, x=10, percent_noise=10):\n",
    "    \"\"\"\n",
    "    The apparent noise one sees in a sensitivity map may be due to \n",
    "    essentially meaningless local variations in partial derivatives.\n",
    "    After all, given typical training techniques there is no reason to expect derivatives to vary smoothly.\n",
    "    \"\"\"\n",
    "    if USE_CUDA:\n",
    "        tensor_input = torch.from_numpy(np.array(input)).type(torch.cuda.FloatTensor) # input is now of shape (w,h,c)\n",
    "    else:\n",
    "        tensor_input = torch.from_numpy(np.array(input)).type(torch.FloatTensor) # input is now of shape (w,h,c)\n",
    "    # x is the sample size\n",
    "    if USE_CUDA:\n",
    "        final_grad = torch.zeros((1,3,224,224)).cuda()\n",
    "    else:\n",
    "        final_grad = torch.zeros((1,3,224,224))\n",
    "    for i in range(x):\n",
    "        print('Sample:', i+1)\n",
    "        temp_input = tensor_input\n",
    "        # According to the paper, noise level corrresponds to stddev/(xmax-xmin). Hence stddev = noise_percentage * (max-min) /100\n",
    "        if USE_CUDA:\n",
    "            noise = torch.from_numpy(np.random.normal(loc=0, scale=(percent_noise/100) * (tensor_input.max() - tensor_input.min()), size=temp_input.shape)).type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            noise = torch.from_numpy(np.random.normal(loc=0, scale=(percent_noise/100) * (tensor_input.max() - tensor_input.min()), size=temp_input.shape)).type(torch.FloatTensor)\n",
    "        temp_input = (temp_input + noise).cpu().numpy()\n",
    "        temp_input = Image.fromarray(temp_input.astype(np.uint8))\n",
    "        if USE_CUDA:\n",
    "            temp_input = Variable(preprocess(temp_input).unsqueeze(0).cuda(), requires_grad=True)\n",
    "        else:\n",
    "            temp_input = Variable(preprocess(temp_input).unsqueeze(0), requires_grad=True)\n",
    "\n",
    "        output = vgg.forward(temp_input)\n",
    "        output[0][label].backward()\n",
    "        final_grad += temp_input.grad.data\n",
    "    \n",
    "    grads = final_grad/x\n",
    "    grads = grads.clamp(min=0)\n",
    "    grads.squeeze_()\n",
    "    grads.transpose_(0,1)\n",
    "    grads.transpose_(1,2)\n",
    "    grads = np.amax(grads.cpu().numpy(), axis=2)\n",
    "    \n",
    "    true_image = normalize(input)\n",
    "    true_image = true_image.squeeze()\n",
    "    true_image = true_image.transpose(0,1)\n",
    "    true_image = true_image.transpose(1,2)\n",
    "    true_image = deprocess(true_image.data)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "\n",
    "\n",
    "    a = fig.add_subplot(1,2,1)\n",
    "    imgplot = plt.imshow(true_image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off') \n",
    "\n",
    "    a = fig.add_subplot(1,2,2)\n",
    "    imgplot = plt.imshow(grads)\n",
    "    plt.axis('off')  \n",
    "    plt.title('SmoothGrad, Noise: ' + str(percent_noise) + '%, ' + 'Samples: ' + str(x))\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_sg = load_image('images/Golden_retr.jpg')\n",
    "dog_sal = make_saliency_map(dog_sg, 1)\n",
    "dog_sg_sal = smooth_grad(dog, 207, 30, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldfish_sg = load_image('images/goldfish.jpg')\n",
    "godlfish_sal = make_saliency_map(goldfish_sg, 1)\n",
    "goldfish_sg_sal = smooth_grad(goldfish, 1, 30, 10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
